{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17195329",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all libraries necessary for pipeline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "import sys\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset or remove any handlers from logger instances\n",
    "logger.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e91b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a directory and file for each logging level\n",
    "\n",
    "log_levels = [\"DEBUG\",\"INFO\", \"WARNING\", \"ERROR\"]\n",
    "for level in log_levels:\n",
    "    os.makedirs(f\"logs/{level.lower()}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb14d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create each logger for each level\n",
    "\n",
    "logger.add(\n",
    "        \"logs/info/info.log\",\n",
    "        level=\"INFO\",\n",
    "       format= \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \" \\\n",
    "        \"PID:{process.id} | TID{thread.id}| \"\n",
    "        \"{message}\",\n",
    "        filter= lambda record: record[\"level\"].name == \"INFO\"\n",
    "    )\n",
    "logger.add(\n",
    "        \"logs/warning/warning.log\",\n",
    "        level=\"WARNING\",\n",
    "       format= \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \" \\\n",
    "        \"PID:{process.id} | TID{thread.id}| \"\n",
    "        \"{message}\",\n",
    "        filter= lambda record: record[\"level\"].name == \"WARNING\"\n",
    "    )\n",
    "logger.add(\n",
    "        \"logs/debug/debug.log\",\n",
    "        level=\"DEBUG\",\n",
    "       format= \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \" \\\n",
    "        \"PID:{process.id} | TID{thread.id}| \"\n",
    "        \"{message}\",\n",
    "        filter= lambda record: record[\"level\"].name == \"DEBUG\"\n",
    "    )\n",
    "logger.add(\n",
    "        \"logs/error/error.log\",\n",
    "        level=\"ERROR\",\n",
    "       format= \"{time:YYYY-MM-DD HH:mm:ss.SSS} | \" \\\n",
    "        \"PID:{process.id} | TID{thread.id}| \"\n",
    "        \"{message}\",\n",
    "        filter= lambda record: record[\"level\"].name == \"ERROR\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load credentials API and database\n",
    "\n",
    "load_success = load_dotenv()\n",
    "try:\n",
    "\n",
    "    if load_success:\n",
    "        logger.info('Successfully loaded environement variables from .env file')\n",
    "    else:\n",
    "        logger.warning('No .env file found or loaded. Continuing without environment variables')\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get credentials for API \n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "API_HOST = \"linkedin-job-search-api.p.rapidapi.com\"\n",
    "try:\n",
    "    if API_KEY is None:\n",
    "        logger.error(\"API key not found in environment variables. Application may not function correctly.\")\n",
    "        raise ValueError\n",
    "    else:\n",
    "        logger.info(\"environmnet credentials were successfully retrieved.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750701a",
   "metadata": {},
   "source": [
    "Extra Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ec4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve data from inkedin-job API using an API request\n",
    "\n",
    "url = \"https://linkedin-job-search-api.p.rapidapi.com/active-jb-7d\"\n",
    "\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": API_KEY,\n",
    "    \"x-rapidapi-host\": API_HOST\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "limit = 100\n",
    "total_needed = 1000\n",
    "\n",
    "logger.info(f\"Starting API fetch with total_needed ={total_needed} and limit = {limit}\")\n",
    "\n",
    "## return a total of 1000 job postings back and offset by 100\n",
    "for offset in range(0, total_needed, limit):\n",
    "    querystring = {\n",
    "        \"limit\": str(limit),\n",
    "        \"offset\": str(offset),\n",
    "        \"title_filter\": \"Data Engineer\",\n",
    "        \"location_filter\": \"United States\",\n",
    "        \"description_type\": \"text\"\n",
    "    }\n",
    "\n",
    "    logger.debug(f\"Fetching data with params: {querystring}\")\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if isinstance(data, dict) and \"data\" in data:\n",
    "            all_results.extend(data[\"data\"])\n",
    "        else:\n",
    "            all_results.extend(data)\n",
    "    else:\n",
    "        logger.error(f\"Error {response.status_code}: {response.text}\")\n",
    "    logger.info(f\"Completed fetch for offset={offset}, limit{limit}, total jobs fetched{all_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check to see if results were found from API request\n",
    "\n",
    "if all_results:\n",
    "    logger.debug(f\"api sample results {json.dumps(all_results[0], indent=4)}\")\n",
    "else:\n",
    "    logger.error(\"No results found in API response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flatten semi-structured JSON data into a flat, tabular format\n",
    "\n",
    "data = pd.json_normalize(all_results)\n",
    "logger.info(\"normalize data into a table\")\n",
    "logger.debug(f\"shape of table{data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acfb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create columns for dataframe\n",
    "\n",
    "columns = ['id','date_posted', 'title', 'organization','locations_derived','cities_derived','employment_type','remote_derived','salary_raw.value.minValue', 'salary_raw.value.maxValue', 'description_text']\n",
    "Data_Jobs_data = data[columns]\n",
    "logger.info(\"table updated with selected columns\")\n",
    "logger.debug(f\"new table {Data_Jobs_data.head(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3a7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>title</th>\n",
       "      <th>organization</th>\n",
       "      <th>locations_derived</th>\n",
       "      <th>cities_derived</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>remote_derived</th>\n",
       "      <th>salary_raw.value.minValue</th>\n",
       "      <th>salary_raw.value.maxValue</th>\n",
       "      <th>description_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1873929964</td>\n",
       "      <td>2025-09-20T21:32:43.954</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Best Job Tool</td>\n",
       "      <td>[United States]</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>About The Company\\n\\nBering Straits Native Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1873849392</td>\n",
       "      <td>2025-09-20T19:53:04</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Vinsys Information Technology Inc</td>\n",
       "      <td>[Seattle, Washington, United States]</td>\n",
       "      <td>[Seattle]</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer II Global Marketing Technology\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1873849728</td>\n",
       "      <td>2025-09-20T19:52:52</td>\n",
       "      <td>Network / Data Center Engineer</td>\n",
       "      <td>Kanak Elite Services</td>\n",
       "      <td>[Abilene, Texas, United States]</td>\n",
       "      <td>[Abilene]</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Role: Data Center Technician\\n\\nLocation Abile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1873849400</td>\n",
       "      <td>2025-09-20T19:52:50</td>\n",
       "      <td>Data Engineer Lead</td>\n",
       "      <td>Shrive Technologies</td>\n",
       "      <td>[Fremont, California, United States]</td>\n",
       "      <td>[Fremont]</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Microsoft Fabric.\\nMicrosoft Azure: Azure Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873849403</td>\n",
       "      <td>2025-09-20T19:52:50</td>\n",
       "      <td>Data Analytical Engineer</td>\n",
       "      <td>Vinsys Information Technology Inc</td>\n",
       "      <td>[Malvern, Pennsylvania, United States]</td>\n",
       "      <td>[Malvern]</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Writes ETL (Extract / Transform / Load) proces...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id              date_posted                           title  \\\n",
       "0  1873929964  2025-09-20T21:32:43.954                   Data Engineer   \n",
       "1  1873849392      2025-09-20T19:53:04                   Data Engineer   \n",
       "2  1873849728      2025-09-20T19:52:52  Network / Data Center Engineer   \n",
       "3  1873849400      2025-09-20T19:52:50              Data Engineer Lead   \n",
       "4  1873849403      2025-09-20T19:52:50        Data Analytical Engineer   \n",
       "\n",
       "                        organization                       locations_derived  \\\n",
       "0                      Best Job Tool                         [United States]   \n",
       "1  Vinsys Information Technology Inc    [Seattle, Washington, United States]   \n",
       "2               Kanak Elite Services         [Abilene, Texas, United States]   \n",
       "3                Shrive Technologies    [Fremont, California, United States]   \n",
       "4  Vinsys Information Technology Inc  [Malvern, Pennsylvania, United States]   \n",
       "\n",
       "  cities_derived employment_type  remote_derived  salary_raw.value.minValue  \\\n",
       "0           None     [FULL_TIME]           False                        NaN   \n",
       "1      [Seattle]     [FULL_TIME]           False                        NaN   \n",
       "2      [Abilene]     [FULL_TIME]           False                        NaN   \n",
       "3      [Fremont]     [FULL_TIME]           False                        NaN   \n",
       "4      [Malvern]     [FULL_TIME]           False                        NaN   \n",
       "\n",
       "   salary_raw.value.maxValue  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "\n",
       "                                    description_text  \n",
       "0  About The Company\\n\\nBering Straits Native Cor...  \n",
       "1  Data Engineer II Global Marketing Technology\\n...  \n",
       "2  Role: Data Center Technician\\n\\nLocation Abile...  \n",
       "3  Microsoft Fabric.\\nMicrosoft Azure: Azure Data...  \n",
       "4  Writes ETL (Extract / Transform / Load) proces...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## preview the new table\n",
    "\n",
    "Data_Jobs_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f71a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "## check date_posted data type\n",
    "\n",
    "print(Data_Jobs_data[\"date_posted\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\218564126.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data[\"date_posted\"] = pd.to_datetime(Data_Jobs_data[\"date_posted\"], errors=\"coerce\")\n",
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\218564126.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data[\"date_posted\"] = pd.to_datetime(Data_Jobs_data[\"date_posted\"]).dt.normalize()\n"
     ]
    }
   ],
   "source": [
    "## change date_posted data type to date\n",
    "\n",
    "Data_Jobs_data[\"date_posted\"] = pd.to_datetime(Data_Jobs_data[\"date_posted\"], errors=\"coerce\")\n",
    "Data_Jobs_data[\"date_posted\"] = pd.to_datetime(Data_Jobs_data[\"date_posted\"]).dt.normalize()\n",
    "\n",
    "if pd.api.types.is_datetime64_any_dtype(Data_Jobs_data[\"date_posted\"]):\n",
    "    logger.info(\n",
    "        f\"'date_posted' column data type successfully changed to {Data_Jobs_data['date_posted'].dtype}\"\n",
    "    )\n",
    "else:\n",
    "    logger.error(\n",
    "        f\"Unsuccessful at changing 'date_posted' data type, current dtype is {Data_Jobs_data['date_posted'].dtype}\"\n",
    "    )\n",
    "    raise ValueError\n",
    "    \n",
    "logger.debug(f\"column date_posted is now displayed as{Data_Jobs_data.head(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed582c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\3482302250.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data[\"cities_derived\"] = Data_Jobs_data[\"cities_derived\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n"
     ]
    }
   ],
   "source": [
    "## change the column cities_derived from a list to a string separated by commas\n",
    "\n",
    "logger.debug(\"Starting transformation of 'cities_derived' column.\")\n",
    "\n",
    "before_dtype = Data_Jobs_data[\"cities_derived\"].dtype\n",
    "\n",
    "logger.info(f\"Original dtype of 'cities_derived': {before_dtype}\")\n",
    "\n",
    "Data_Jobs_data[\"cities_derived\"] = Data_Jobs_data[\"cities_derived\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "after_dtype = Data_Jobs_data[\"cities_derived\"].dtype\n",
    "\n",
    "logger.info(f\"Transformation successful. New dtype of 'cities_derived': {after_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1    Seattle\n",
       "2    Abilene\n",
       "3    Fremont\n",
       "4    Malvern\n",
       "Name: cities_derived, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## preview cities_derived column\n",
    "\n",
    "Data_Jobs_data[\"cities_derived\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the first text for employment_type\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['employment_type'].head(5)}\")\n",
    "Data_Jobs_data['employment_type'] = Data_Jobs_data['employment_type'].str[0]\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['employment_type'].head(5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db434754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\2985135592.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data['salary_raw.value.minValue'] = Data_Jobs_data['salary_raw.value.minValue'].apply(lambda x: \"${:,.2f}\".format(x) if pd.notnull(x) else x)\n"
     ]
    }
   ],
   "source": [
    "## change format of salary_raw.value.minValue from decimal to currency\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['salary_raw.value.minValue'].head(2)}\")\n",
    "Data_Jobs_data['salary_raw.value.minValue'] = Data_Jobs_data['salary_raw.value.minValue'].apply(lambda x: \"${:,.2f}\".format(x) if pd.notnull(x) else x)\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['salary_raw.value.minValue'].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf41bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\3536499586.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data['salary_raw.value.maxValue'] = Data_Jobs_data['salary_raw.value.maxValue'].apply(lambda x: \"${:,.2f}\".format(x) if pd.notnull(x) else x)\n"
     ]
    }
   ],
   "source": [
    "## change format of salary_raw.value.maxValue from decimal  to currency\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['salary_raw.value.maxValue'].head(2)}\")\n",
    "Data_Jobs_data['salary_raw.value.maxValue'] = Data_Jobs_data['salary_raw.value.maxValue'].apply(lambda x: \"${:,.2f}\".format(x) if pd.notnull(x) else x)\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['salary_raw.value.maxValue'].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb738f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NDickerson\\AppData\\Local\\Temp\\ipykernel_98380\\1547061746.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Data_Jobs_data['locations_derived'] = Data_Jobs_data['locations_derived'].str[0]\n"
     ]
    }
   ],
   "source": [
    "## turn locations derived column into a string\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['locations_derived'].head(2)}\")\n",
    "Data_Jobs_data['locations_derived'] = Data_Jobs_data['locations_derived'].str[0]\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['locations_derived'].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split values in locations_derived into separate columns  and drop columns that are not states\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['locations_derived'].head(2)}\")\n",
    "Data_Jobs_data_expanded = Data_Jobs_data['locations_derived'].str.split(',', expand = True)\n",
    "Data_Jobs_data_expanded_with = Data_Jobs_data.join(Data_Jobs_data_expanded)\n",
    "Data_Jobs_data_expanded_with\n",
    "Data_Jobs_data = Data_Jobs_data_expanded_with.drop([2, 0], axis=1)\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['locations_derived'].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed98f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## permantely change the column that have states to the name state\n",
    "\n",
    "Data_Jobs_data.rename(columns={1: 'state'}, inplace= True)\n",
    "logger.debug(f\"new column created {Data_Jobs_data['state'].head(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create new column names for dataframe that includes the column state\n",
    "\n",
    "columns = ['id','date_posted','title','organization','locations_derived','state','cities_derived','employment_type','remote_derived','salary_raw.value.minValue','salary_raw.value.maxValue','description_text']\n",
    "Data_Jobs_data[columns]\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "logger.debug(f\"Dataframe with updated columns {Data_Jobs_data.head(4)}\")\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract only the state from the locations_derived table\n",
    "def extract_state(location):\n",
    "    if pd.isna(location):  # handle NaN\n",
    "        return None\n",
    "    parts = location.split(\",\")\n",
    "    if len(parts) == 3:\n",
    "        return parts[1].strip()\n",
    "    elif len(parts) == 2:\n",
    "        return parts[0].strip()\n",
    "    else:\n",
    "        return location.strip()\n",
    "\n",
    "Data_Jobs_data[\"state\"] = Data_Jobs_data[\"locations_derived\"].apply(extract_state)\n",
    "logger.debug(f\"dataframe after extracting state {Data_Jobs_data[\"state\"].head(2)}\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the locations_derived column\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['locations_derived'].head(2)}\")\n",
    "Data_Jobs_data = Data_Jobs_data.drop('locations_derived', axis= 1)\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data.head(1)}\")\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff12fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert state column right before cities column\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['state'].head(2)}\")\n",
    "cols = list(Data_Jobs_data.columns)\n",
    "cols.insert(4, cols.pop(cols.index('state')))\n",
    "Data_Jobs_data = Data_Jobs_data[cols]\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data['state'].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change description_text data type from an object to a string\n",
    "\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['description_text'].dtype}\")\n",
    "Data_Jobs_data['description_text'] = Data_Jobs_data['description_text'].astype('string')\n",
    "after_dtype = Data_Jobs_data[\"description_text\"].dtype\n",
    "logger.debug(f\"'description_text' dtype after transformation: {after_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate each individual skill by a comma for each row\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "skills_list = [\n",
    "    \"python\", \"sql\", \"spark\", \"aws\", \"azure\", \"snowflake\", \"data processing\",\n",
    "    \"data storage\", \"data management\", \"data ingestion\", \"data preparation\",\n",
    "    \"data provisioning\", \"real-time processing\", \"informatica\",\n",
    "    \"bachelor's degree\", \"engineering\", \"mathematics\", \"computer science\",\n",
    "    \"ai/ml\", \"ci/cd\", \"postgres\", \"nosql\", \"data warehouse\", \"rdbms\",\n",
    "    \"datalake\", \"github\", \"devops\", \"mapreduce\", \"hive\", \"emr\", \"kafka\",\n",
    "    \"gurobi\", \"docker\", \"kubernetes\", \"big data\", \"c++\", \"javascript\",\n",
    "    \"cassandra\", \"pandas\", \"data pipelines\", \"java\", \"lake houses\",\n",
    "    \"apache iceberg\", \"tableau\", \"power bi\", \"data modelling\",\n",
    "    \"data modeling\", \"data models\", \"apache beam\", \"bigquery\", \"gcp\",\n",
    "    \"machine learning\", \"google cloud\", \"data pipeline\", \"fabric\",\n",
    "    \"data warehousing\", \"linux\", \"windows\", \"unix\", \"databricks\", \"etl\"\n",
    "]\n",
    "logger.debug(f\"dataframe before transformation{Data_Jobs_data['description_text'].head(1)}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
    "    return text\n",
    "\n",
    "def extract_skills(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    found = []\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        # word boundary match if it's a single word\n",
    "        if \" \" not in skill and \"/\" not in skill and \"-\" not in skill:\n",
    "            if re.search(rf\"\\b{re.escape(skill)}\\b\", text):\n",
    "                found.append(skill)\n",
    "        else:\n",
    "            # direct substring match for multi-word or special cases\n",
    "            if skill in text:\n",
    "                found.append(skill)\n",
    "    \n",
    "    return found if found else None\n",
    "Data_Jobs_data[\"skills_requirements\"] = Data_Jobs_data[\"description_text\"].apply(extract_skills)\n",
    "logger.debug(f\"dataframe after transformation{Data_Jobs_data[\"skills_requirements\"].head(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21130d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the column description_text\n",
    "\n",
    "logger.debug(f\"dataframe before dropping description_text column{Data_Jobs_data.columns}\")\n",
    "Data_Jobs_data = Data_Jobs_data.drop('description_text', axis= 1)\n",
    "logger.debug(f\"dataframe after dropping description_text column{Data_Jobs_data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d250a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## join all skills together for each posting\n",
    "\n",
    "logger.debug(f\"dataframe before skills requirements transformation{Data_Jobs_data[\"skills_requirements\"].head(2)}\")\n",
    "Data_Jobs_data[\"skills_requirements\"] = (\n",
    "    Data_Jobs_data[\"skills_requirements\"]\n",
    "    .apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    ")\n",
    "logger.debug(f\"dataframe after skills requirements transformation{Data_Jobs_data[\"skills_requirements\"].head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe12ae76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>title</th>\n",
       "      <th>organization</th>\n",
       "      <th>state</th>\n",
       "      <th>cities_derived</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>remote_derived</th>\n",
       "      <th>salary_raw.value.minValue</th>\n",
       "      <th>salary_raw.value.maxValue</th>\n",
       "      <th>skills_requirements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1873929964</td>\n",
       "      <td>2025-09-20</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Best Job Tool</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python, sql, spark, data management, engineeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1873849392</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Vinsys Information Technology Inc</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python, sql, spark, azure, snowflake, data man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1873849728</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Network / Data Center Engineer</td>\n",
       "      <td>Kanak Elite Services</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1873849400</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Data Engineer Lead</td>\n",
       "      <td>Shrive Technologies</td>\n",
       "      <td>California</td>\n",
       "      <td>Fremont</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python, sql, spark, azure, engineering, kafka,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873849403</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Data Analytical Engineer</td>\n",
       "      <td>Vinsys Information Technology Inc</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Malvern</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python, sql, spark, aws, data processing, gith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1872288869</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Data Engineer - Senior Manager</td>\n",
       "      <td>PwC</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>$124,000.00</td>\n",
       "      <td>$280,000.00</td>\n",
       "      <td>aws, azure, snowflake, data processing, engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1872288831</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Data Engineer - Senior Manager</td>\n",
       "      <td>PwC</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>$124,000.00</td>\n",
       "      <td>$280,000.00</td>\n",
       "      <td>aws, azure, snowflake, data processing, engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1872289378</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Mechanical Engineer - Data Center Components</td>\n",
       "      <td>Hyper Solutions</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1872288075</td>\n",
       "      <td>NaT</td>\n",
       "      <td>GCP Data Engineer - Senior Manager</td>\n",
       "      <td>PwC</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>$124,000.00</td>\n",
       "      <td>$280,000.00</td>\n",
       "      <td>sql, spark, snowflake, data processing, data s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1872288091</td>\n",
       "      <td>NaT</td>\n",
       "      <td>GCP Data Engineer - Manager</td>\n",
       "      <td>PwC</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>[FULL_TIME]</td>\n",
       "      <td>False</td>\n",
       "      <td>$99,000.00</td>\n",
       "      <td>$232,000.00</td>\n",
       "      <td>sql, spark, snowflake, data processing, data s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id date_posted                                         title  \\\n",
       "0    1873929964  2025-09-20                                 Data Engineer   \n",
       "1    1873849392         NaT                                 Data Engineer   \n",
       "2    1873849728         NaT                Network / Data Center Engineer   \n",
       "3    1873849400         NaT                            Data Engineer Lead   \n",
       "4    1873849403         NaT                      Data Analytical Engineer   \n",
       "..          ...         ...                                           ...   \n",
       "995  1872288869         NaT                Data Engineer - Senior Manager   \n",
       "996  1872288831         NaT                Data Engineer - Senior Manager   \n",
       "997  1872289378         NaT  Mechanical Engineer - Data Center Components   \n",
       "998  1872288075         NaT            GCP Data Engineer - Senior Manager   \n",
       "999  1872288091         NaT                   GCP Data Engineer - Manager   \n",
       "\n",
       "                          organization           state cities_derived  \\\n",
       "0                        Best Job Tool            None           None   \n",
       "1    Vinsys Information Technology Inc      Washington        Seattle   \n",
       "2                 Kanak Elite Services           Texas        Abilene   \n",
       "3                  Shrive Technologies      California        Fremont   \n",
       "4    Vinsys Information Technology Inc    Pennsylvania        Malvern   \n",
       "..                                 ...             ...            ...   \n",
       "995                                PwC   United States           None   \n",
       "996                                PwC   United States           None   \n",
       "997                    Hyper Solutions        Virginia           None   \n",
       "998                                PwC   United States           None   \n",
       "999                                PwC   United States           None   \n",
       "\n",
       "    employment_type  remote_derived salary_raw.value.minValue  \\\n",
       "0       [FULL_TIME]           False                       NaN   \n",
       "1       [FULL_TIME]           False                       NaN   \n",
       "2       [FULL_TIME]           False                       NaN   \n",
       "3       [FULL_TIME]           False                       NaN   \n",
       "4       [FULL_TIME]           False                       NaN   \n",
       "..              ...             ...                       ...   \n",
       "995     [FULL_TIME]           False               $124,000.00   \n",
       "996     [FULL_TIME]           False               $124,000.00   \n",
       "997     [FULL_TIME]           False                       NaN   \n",
       "998     [FULL_TIME]           False               $124,000.00   \n",
       "999     [FULL_TIME]           False                $99,000.00   \n",
       "\n",
       "    salary_raw.value.maxValue  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "3                         NaN   \n",
       "4                         NaN   \n",
       "..                        ...   \n",
       "995               $280,000.00   \n",
       "996               $280,000.00   \n",
       "997                       NaN   \n",
       "998               $280,000.00   \n",
       "999               $232,000.00   \n",
       "\n",
       "                                   skills_requirements  \n",
       "0    python, sql, spark, data management, engineeri...  \n",
       "1    python, sql, spark, azure, snowflake, data man...  \n",
       "2                                                 None  \n",
       "3    python, sql, spark, azure, engineering, kafka,...  \n",
       "4    python, sql, spark, aws, data processing, gith...  \n",
       "..                                                 ...  \n",
       "995  aws, azure, snowflake, data processing, engine...  \n",
       "996  aws, azure, snowflake, data processing, engine...  \n",
       "997                                        engineering  \n",
       "998  sql, spark, snowflake, data processing, data s...  \n",
       "999  sql, spark, snowflake, data processing, data s...  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"dataframe transformation complete\")\n",
    "Data_Jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfcbdfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linkedin_jobs\n"
     ]
    }
   ],
   "source": [
    "SQL_SERVER_DATABASE = \"Linkedin_jobs\"\n",
    "SQL_SERVER_HOST = os.getenv(\"SQL_SERVER_HOST\")\n",
    "SQL_SERVER_DATABASE = os.getenv(\"SQL_SERVER_DATABASE\")\n",
    "print(SQL_SERVER_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a12d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Microsoft SQL Server 2022 (RTM-CU20-GDR) (KB5065220) - 16.0.4212.1 (X64) \\n\\tAug 13 2025 16:40:40 \\n\\tCopyright (C) 2022 Microsoft Corporation\\n\\tDeveloper Edition (64-bit) on Windows 10 Enterprise 10.0 <X64> (Build 26100: ) (Hypervisor)\\n',)\n"
     ]
    }
   ],
   "source": [
    "## create connection to SQL SERVER\n",
    "\n",
    "SQL_SERVER_HOST = \"LPJZ5KFY3\"\n",
    "SQL_SERVER_DATABASE = \"Linkedin_jobs\"\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    f\"SERVER={SQL_SERVER_HOST};\"\n",
    "    f\"DATABASE={SQL_SERVER_DATABASE};\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT @@VERSION;\")\n",
    "print(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create table in Linkedin_jobs database\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "IF NOT EXISTS (\n",
    "    SELECT 1 FROM sysobjects \n",
    "    WHERE name = 'Linkedin_jobs' AND xtype = 'U'\n",
    ")\n",
    "BEGIN\n",
    "\n",
    "    CREATE TABLE Linkedin_jobs(\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        date_posted DATE,\n",
    "        title varchar(300),\n",
    "        organization varchar(300),\n",
    "        state varchar(300),\n",
    "        cities_derived varchar(300),\n",
    "        employment_type varchar(300),\n",
    "        remote_derived BIT DEFAULT 0,\n",
    "        salary_raw_value_minValue MONEY, \n",
    "        salary_raw_value_maxValue MONEY,\n",
    "        skills_requirements varchar(MAX)\n",
    "    );\n",
    "END\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd04e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create SQL SERVER engine\n",
    "\n",
    "engine = create_engine(f'mssql+pyodbc://{SQL_SERVER_HOST}/{SQL_SERVER_DATABASE}?driver=ODBC+Driver+17+for+SQL+Server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "666e9a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully inserted into Linkedin_jobs.\n"
     ]
    }
   ],
   "source": [
    "# Ensure a copy (avoids SettingWithCopyWarning)\n",
    "Data_Jobs_data = Data_Jobs_data.copy()\n",
    "\n",
    "# Rename columns to match SQL Server table\n",
    "Data_Jobs_data = Data_Jobs_data.rename(columns={\n",
    "    \"salary_raw.value.minValue\": \"salary_raw_value_minValue\",\n",
    "    \"salary_raw.value.maxValue\": \"salary_raw_value_maxValue\"\n",
    "})\n",
    "\n",
    "# Also make sure 'employment_type' is a string, not a list\n",
    "if Data_Jobs_data[\"employment_type\"].apply(lambda x: isinstance(x, list)).any():\n",
    "    Data_Jobs_data[\"employment_type\"] = Data_Jobs_data[\"employment_type\"].apply(\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )\n",
    "\n",
    "# Insert into SQL Server\n",
    "table_name = 'Linkedin_jobs'\n",
    "try:\n",
    "    Data_Jobs_data.to_sql(table_name, engine, if_exists='append', index=False, chunksize=1000)\n",
    "    print(f\"✅ Data successfully inserted into {table_name}.\")\n",
    "    logger.info(\"data successfully loaded into database\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error inserting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b24055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
